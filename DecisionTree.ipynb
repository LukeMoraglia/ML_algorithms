{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf290a7-362d-4ebf-bf52-8bc9e05b82c5",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision tree is a supervised prediction algorithm whose main idea is to split the training observations into several regions, called leaves, through a series of splits, with each step splitting the observations using a single predictor variable. Trees can use both continuous and categorical predictors, and the outcome variable determines if the tree is a regression tree or a classification tree. \n",
    "\n",
    "## Regression Tree\n",
    "To build a regression tree we need to create a series of splits on the observations. We start with a \"root\" node which represents the first split of the data, and we need to choose a single variable to split at a specific split point. The split point could be a specific cutoff value for a continous variable or a specific level of a categorical variable. To find the best split, we take a greedy approach where we make each split so that it minimizes the residual sum of squares. The residual is the difference between an observation's outcome value and the prediction given by the tree, where the prediction is the average of all the observations in the leaf. \n",
    "\n",
    "## Classification Tree\n",
    "For classification trees, the prediction will be the most commonly occurring level of the outcome variable in the leaf, so we can't use the residual sum of squares and need a different metric. One option is the Gini index, which approaches zero as the leaf gains a larger majority of a single level of the outcome variable, and is exactly zero when the leaf only has a single level in it. It is also sometimes called Gini \"impurity\" because a zero value is a very \"pure\" leaf. Another option is entropy, which has a very similar but slightly different formula to Gini, but has the same trends where it approaches zero the more pure the leaf is. \n",
    "\n",
    "## Hyperparameters\n",
    "Trees have several hyperparameters, many of which you can see being tuned in tree ensemble algorithms like random forest and gradient boosting. Some examples are:\n",
    "- Max depth: the maximum number of splits you can go through to get to the terminal leaf\n",
    "- Max leaf nodes: the maximum number of terminal leaf nodes\n",
    "- Min samples split: the minimum number of observations that need to go to either side of a split\n",
    "- Many many more, as always read the docs :)\n",
    "\n",
    "## Pros and Cons\n",
    "Pros:\n",
    "- May outperform \"classical\" methods such as linear regression, especially for non-linear complex relationships between the predictors and outcome. \n",
    "- Easy interpretation and visualization which can be helpful for non-experts in ML.\n",
    "- Able to handle categorical predictors without one-hot encoding or other transformations.\n",
    "\n",
    "Cons:\n",
    "- Trees can easily overfit the data. Tricks like pruning can help because smaller trees will have lower variance while adding higher bias. \n",
    "- Trees on their own are usually outperformed by other ensemble tree methods, so single trees are not usually implemented. \n",
    "- They can be non-robust, with small changes in training data changing the trees. Once again, this can be improved using ensemble techniques like bagging, boosting, and random forests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1435dc3e-0470-4353-b390-d50aed7619c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "venv_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
